# Airbnb 덴버 데이터셋 분석
이 데이터는 Airbnb의 덴버지역 숙소 데이터로, 38개의 속성을 가진 데이터다.

`1. 데이터 전처리`에서는 판다스를 활용해 이 데이터의 결측치를 지우거나 채우고, 별로 중요하지 않은 데이터를 지우는 작업을 했다.

`2.슈퍼호스트 예측`에서는 판다스를 이용해 이 데이터를 딥러닝에 용이하게 수정한 후, 슈퍼호스트 정보가 누락된 행들에 DNN 기반으로 예측하여 결측치를 채워넣었다.

결과적으로, `listing_Denvor_2.csv`를 만들어 냈고, 이를 기반으로 데이터를 분석하여 insightf를 도출할 수 있을 것이다.

## 리뷰
이 데이터는 세종대 데이터사이언스과 공학설계기초 과목 기말 대체 과제로 출제되었다...

나는 어차피 과제도 아니라서 데이터 cleaning과 결손치 처리, 딥러닝 등에 초점을 맞추었지만, 더욱 발전해 보고 싶거나 과제라서 빡세게 해야겠다 싶으면 아래를 참조하면 좋을 것이다.

1. 데이터 전처리 과정에서, 나는 아래와 같은 과정을 행했다.
    - host_name 은 'dummy name'으로 채우기
    - host_since는 first_review로 채우고 둘다 null이면 삭제, DL 용이하도록 2019년에서 빼서 운영 기간으로 변환
    - square_feet는 거의 전체가 null
    - first_review와 last_review는 부정확한 데이터를 채워가며 복원하기엔 중요도가 떨어짐
    - review_scores_rating과 review_per_month는 중요한 데이터인데 missing인 경우, 추천이 힘드므로, 복원하기보단 이 행들이 null인 열을 삭제
    - cleaning_fee는 삭제
    - host_is_superhost는 딥러닝하여 추론(다른 파일에서 할 것임)
    - zipcode도 별로 중요치 않으므로 삭제
    - host_response 관련 데이터는 숙소 추천에 아주 중요하지는 않으므로 삭제, host_name도 중요도 떨어지므로 삭제
    - market 결손치는 city로 채우기
    - city 는 host_location 잘라서 채우기
    - state는 CO 밖에 없으니까 삭제
    - host_location은 US, CO 까지 모두 동일하며 도시 정보는 city에 있으므로 삭제
    - bathrooms는 중요할 수 있고, 해당 데이터가 없는 행이 얼마 없으므로 행을 삭제
    
2. 이 때, 아래와 같은 사항을 고려할 수 있다.

    host_since는 first_review 로 채웠는데, 이 때, 다른 행들의 first_review-host_since 값의 평균을 내서 적용하면 host_since에 가까운 예측을 할 수 있다.
    
    운영기간으로 변환할 때, 연도에서 빼면 간단하지만 개월 단위로 했으면 조금 더 자세한 정보가 되었을 수 있다.
    
    square_feet같은 정확한 넓이 정보가 없을 때, 화장실 개수와 침실 개수를 활용해서 아예 숙소 크기라는 새로운 지표로 만드는 것도 방법이 될 수 있다.
    
    first_review, last_review는 디테일한 정보로 복원하기 어렵다. 다만, last review는, 해당 호스트의 최근 활동 일자를 알 수 있어 함부로 날리기에 아쉬운 자료이긴 하다. 기회가 되면 복원하거나 단편적으로 활용할 방안을 생각해보자.
    
    review_scores_rating 등 리뷰 관련 정보는 매우 중요도가 높은데, 복원하기도 까다롭다. 어쩔 수 없이 missing row를 지우는 것이 가장 적절한 대처가 될 수 있다.
    
    cleaning_fee도 귀찮아서 일단 삭제했지만, 평균을 내서 채우는 것이 방법이 될 수 있다. 이때, 위에서 말한 숙소 크기 지표를 만들어 활용하면 더 나은 정확도를 가질 수 있다.
    
    host_is_superhost 는 단순한 이진 분류 문제로,  예측을 위한 딥러닝 코드 구현이 어렵지 않다. 이런 경우 정확도 0.5 이상의 DL 모델 구현이 가능하면 구현해서 쓰는게 좋을 수 있다.
    
    market 결손치를 비롯하여 위치 정보들은 다른 값을 참조해서 매울 수 있다.
    
3. 이 외에도 나는 시간이 아까워 생략한 부분이 많다. 대표적으로, 소수 데이터에 대한 오타 검증 등을 하지 않았고, 데이터 교차 검증도 생략했다. 예를 들어, state와 host_location의 정보가 일치하지 않는 정보가 있는지와 같은 사항을 체크하면 좋다.

4. 딥러닝 과정에서 수많은 데이터를 무관하다고 판단하고 날렸는데, 사실 관련이 있을 수도 있다.